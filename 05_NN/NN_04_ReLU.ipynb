{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TENSORBOARD USED\n",
    "- tensorboard --logdir =./logs\n",
    "- more DeeeeeeeeeeP\n",
    "- using relu - last sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 2], name='x-input')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name='y-input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer1\") as scope:\n",
    "    W1 = tf.Variable(tf.random_normal([2,5]),name=\"Weight_one\")\n",
    "    b1 = tf.Variable(tf.random_normal([5]),name=\"Bias_one\")\n",
    "    layer_one = tf.nn.relu(tf.matmul(X,W1) + b1)\n",
    "    \n",
    "    w1_histogram = tf.summary.histogram(\"Weight_one\", W1)\n",
    "    b1_histogram = tf.summary.histogram(\"Bias-one\",b1)\n",
    "    layer_one_histogram = tf.summary.histogram(\"layer1\",layer_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more Deep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    W2 = tf.Variable(tf.random_normal([5,5]),name=\"Weight_two\")\n",
    "    b2 = tf.Variable(tf.random_normal([5]),name=\"Bias_two\")\n",
    "    layer_two = tf.nn.relu(tf.matmul(layer_one,W2) + b2)\n",
    "    \n",
    "    w2_histogram = tf.summary.histogram(\"Weight_two\", W2)\n",
    "    b2_histogram = tf.summary.histogram(\"Bias_two\",b2)\n",
    "    layer_two_histogram = tf.summary.histogram(\"layer2\",layer_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer3\") as scope:\n",
    "    W3 = tf.Variable(tf.random_normal([5,5]),name=\"Weight_three\")\n",
    "    b3 = tf.Variable(tf.random_normal([5]),name=\"Bias_three\")\n",
    "    layer_three = tf.nn.relu(tf.matmul(layer_two,W3) + b3)\n",
    "    \n",
    "    w3_histogram = tf.summary.histogram(\"Weight_three\", W3)\n",
    "    b3_histogram = tf.summary.histogram(\"Bias_three\",b3)\n",
    "    layer_three_histogram = tf.summary.histogram(\"layer3\",layer_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer4\") as scope:\n",
    "    W4 = tf.Variable(tf.random_normal([5,5]),name=\"Weight_four\")\n",
    "    b4 = tf.Variable(tf.random_normal([5]),name=\"Bias_four\")\n",
    "    layer_four = tf.nn.relu(tf.matmul(layer_three,W4) + b4)\n",
    "    \n",
    "    w4_histogram = tf.summary.histogram(\"Weight_four\", W4)\n",
    "    b4_histogram = tf.summary.histogram(\"Bias_four\",b4)\n",
    "    layer_two_histogram = tf.summary.histogram(\"layer4\",layer_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer5\") as scope:\n",
    "    W5 = tf.Variable(tf.random_normal([5,5]),name=\"Weight_five\")\n",
    "    b5 = tf.Variable(tf.random_normal([5]),name=\"Bias_five\")\n",
    "    layer_five = tf.nn.relu(tf.matmul(layer_four,W5) + b5)\n",
    "    \n",
    "    w5_histogram = tf.summary.histogram(\"Weight_five\", W5)\n",
    "    b5_histogram = tf.summary.histogram(\"Bias_five\",b5)\n",
    "    layer_five_histogram = tf.summary.histogram(\"layer_five\",layer_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer6\") as scope:\n",
    "    W6 = tf.Variable(tf.random_normal([5,5]),name=\"Weight_six\")\n",
    "    b6 = tf.Variable(tf.random_normal([5]),name=\"Bias_six\")\n",
    "    layer_six = tf.nn.relu(tf.matmul(layer_five,W6) + b6)\n",
    "    \n",
    "    w6_histogram = tf.summary.histogram(\"Weight_six\", W6)\n",
    "    b6_histogram = tf.summary.histogram(\"Bias_six\",b6)\n",
    "    layer_six_histogram = tf.summary.histogram(\"layer_six\",layer_six)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer7\") as scope:\n",
    "    W7 = tf.Variable(tf.random_normal([5,5]),name=\"Weight_seven\")\n",
    "    b7 = tf.Variable(tf.random_normal([5]),name=\"Bias_seven\")\n",
    "    layer_seven = tf.nn.relu(tf.matmul(layer_six, W7) + b7)\n",
    "    \n",
    "    w7_histogram = tf.summary.histogram(\"Weight_seven\", W7)\n",
    "    b7_histogram = tf.summary.histogram(\"Bias_seven\",b7)\n",
    "    layer_seven_histogram = tf.summary.histogram(\"layer_seven\",layer_seven)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer8\") as scope:\n",
    "    W8 = tf.Variable(tf.random_normal([5,5]),name=\"Weight_eight\")\n",
    "    b8 = tf.Variable(tf.random_normal([5]),name=\"Bias_eight\")\n",
    "    layer_eight = tf.nn.relu(tf.matmul(layer_seven, W8) + b8)\n",
    "    \n",
    "    w8_histogram = tf.summary.histogram(\"Weight_eight\", W8)\n",
    "    b8_histogram = tf.summary.histogram(\"Bias_eight\",b8)\n",
    "    layer_eight_histogram = tf.summary.histogram(\"layer_eight\",layer_eight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer9\") as scope:\n",
    "    W9 = tf.Variable(tf.random_normal([5,5]),name=\"Weight_nine\")\n",
    "    b9 = tf.Variable(tf.random_normal([5]),name=\"Bias_nine\")\n",
    "    layer_nine = tf.nn.relu(tf.matmul(layer_eight, W9) + b9)\n",
    "    \n",
    "    w9_histogram = tf.summary.histogram(\"Weight_nine\", W9)\n",
    "    b9_histogram = tf.summary.histogram(\"Bias-nine\", b9)\n",
    "    layer_nine_histogram = tf.summary.histogram(\"layer_nine\", layer_nine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer10\") as scope:\n",
    "    W10 = tf.Variable(tf.random_normal([5,5]),name=\"Weight_ten\")\n",
    "    b10 = tf.Variable(tf.random_normal([5]),name=\"Bias_ten\")\n",
    "    layer_ten =tf.nn.relu(tf.matmul(layer_nine, W10) + b10)\n",
    "    \n",
    "    w10_histogram = tf.summary.histogram(\"Weight_ten\", W10)\n",
    "    b10_histogram = tf.summary.histogram(\"Bias_ten\",b10)\n",
    "    layer_ten_histogram = tf.summary.histogram(\"layer_ten\",layer_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"last\") as scope:\n",
    "    W11 = tf.Variable(tf.random_normal([5,2]),name=\"Weight_11\")\n",
    "    b11 = tf.Variable(tf.random_normal([2]),name=\"Bias_11\")\n",
    "    Hypothesis = tf.sigmoid(tf.matmul(layer_ten, W11) + b11)\n",
    "    layer_two_histogram = tf.summary.histogram(\"Hypothesis\",Hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = -tf.reduce_mean(Y * tf.log(Hypothesis) + (1 - Y) * tf.log(1 - Hypothesis))\n",
    "    cost_summ = tf.summary.scalar(\"cost\", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR 변경 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\") as scope:\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = tf.cast(Hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "accuracy_summ = tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"\\\\xor_logs_04\")\n",
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.71117\n",
      "1000 0.531699\n",
      "2000 0.477485\n",
      "3000 0.477416\n",
      "4000 0.477403\n",
      "5000 0.477396\n",
      "6000 0.477396\n",
      "7000 0.477393\n",
      "8000 0.477391\n",
      "9000 0.47739\n",
      "10000 0.47739\n"
     ]
    }
   ],
   "source": [
    "for step in range(10001):\n",
    "        summary, _ = sess.run([merged_summary, train], feed_dict={X: x_data, Y: y_data})\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis:  [[  1.55959406e-05   1.95405446e-05]\n",
      " [  6.66585624e-01   6.66564941e-01]\n",
      " [  6.66585624e-01   6.66564941e-01]\n",
      " [  6.66585624e-01   6.66564941e-01]] \n",
      "Correct:  [[ 0.  0.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]] \n",
      "Accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "h, c, a = sess.run([Hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- ReLU를 적용하여 좀더 개선된 정확도를 얻었다. (75%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
